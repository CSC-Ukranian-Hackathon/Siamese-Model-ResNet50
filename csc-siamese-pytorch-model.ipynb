{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, folder1, folder2, dataframe):\n",
    "        self.folder1 = folder1\n",
    "        self.folder2 = folder2\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = ToTensor()\n",
    "\n",
    "        self.image_pairs = self.get_image_pairs()\n",
    "\n",
    "    def get_image_pairs(self):\n",
    "        image_pairs = []\n",
    "        for image_name in os.listdir(self.folder1):\n",
    "            if image_name.endswith(\".jpeg\"):\n",
    "                image_name_without_ext = os.path.splitext(image_name)[0]\n",
    "                image_path1 = os.path.join(self.folder1, image_name).replace(\"\\\\\", \"/\")\n",
    "                image_path2 = os.path.join(self.folder2, image_name).replace(\"\\\\\", \"/\")\n",
    "                # image_path1 = os.path.join(self.folder1, image_name)\n",
    "                # image_path2 = os.path.join(self.folder2, image_name)\n",
    "                is_same = self.dataframe.loc[int(image_name_without_ext), \"is_same\"]\n",
    "                image_pairs.append((image_path1, image_path2, is_same, image_name))\n",
    "        return image_pairs\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path1, image_path2, is_same, image_name = self.image_pairs[index]\n",
    "        image1 = self.transform(Image.open(image_path1))\n",
    "        image2 = self.transform(Image.open(image_path2))\n",
    "\n",
    "        # Apply resize_transform to ensure consistent image size\n",
    "        image1 = resize_transform(image1)\n",
    "        image2 = resize_transform(image2)\n",
    "\n",
    "\n",
    "        return image1, image2, is_same, image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the dataset\n",
    "loaded_dataset = torch.load('I:/CSC Hackathon/siamese_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the Siamese network loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, embedding1, embedding2, target):\n",
    "        euclidean_distance = nn.functional.pairwise_distance(embedding1, embedding2)\n",
    "        loss_contrastive = torch.mean((1 - target) * torch.pow(euclidean_distance, 2) +\n",
    "                                       target * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        self.embedding_size = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        embedding1 = self.backbone(x1)\n",
    "        embedding2 = self.backbone(x2)\n",
    "        return embedding1, embedding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prepare the data loaders\n",
    "resize_transform = Resize((600, 800))\n",
    "batch_size = 8 # 32\n",
    "data_loader = DataLoader(loaded_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Step 5: Train the Siamese network\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss(margin=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 # change it to 10 / 20\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        images1, images2, targets, _ = batch\n",
    "        images1 = images1.to(device)\n",
    "        images2 = images2.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings1, embeddings2 = model(images1, images2)\n",
    "        loss = criterion(embeddings1, embeddings2, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Batch Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
